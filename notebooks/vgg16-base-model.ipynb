{
  "cells": [
    {
      "metadata": {
        "_uuid": "687841669c07ad58d946526e44d7192860bb2a14"
      },
      "cell_type": "markdown",
      "source": "# Summary\n\n[VGG16](https://keras.io/applications/#vgg16) Keras pretrained base case\n\nData pipeline and mask functions are from this notebook: \n\nhttps://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification\n\nIf running on Kaggle, make sure that this notebook is **internet connected**, so the VGG16 weights can be downloaded"
    },
    {
      "metadata": {
        "_uuid": "fa525d982fdb7e518de47cd8ec52f6ee868d312b"
      },
      "cell_type": "markdown",
      "source": "# Changelog\n- Changed uncov to uconv, but removed the dropout in the last layer\n- Corrected sanity check of predicted validation data (changed from ids_train to ids_valid)\n- Used correct mask (from original train_df) for threshold tuning (inserted y_valid_ori)"
    },
    {
      "metadata": {
        "_uuid": "ada861a85e9549dca27667692da408c5fdccbaa5"
      },
      "cell_type": "markdown",
      "source": "# About\nSince I am new to learning from image segmentation and kaggle in general I want to share my noteook.\nI saw it is similar to others as it uses the U-net approach. I want to share it anyway because:\n\n- As said, the field is new to me so I am open to suggestions.\n- It visualizes some of the steps, e.g. scaling, to learn if the methods do what I expect which might be useful to others (I call them sanity checks).\n- Added stratification by the amount of salt contained in the image.\n- Added augmentation by flipping the images along the y axes (thanks to the forum for clarification).\n- Added dropout to the model which seems to improve performance."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport keras\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\nfrom tqdm import tqdm_notebook",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "962c2c6775b5fcf605df8e7c59cbcabe6ba9ceaa"
      },
      "cell_type": "markdown",
      "source": "# Params and helpers"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e54e151245d665e42bb95d9cf2e1a33cb9440e48"
      },
      "cell_type": "code",
      "source": "img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "530c358f2868a444e8233936996463a66c2cc4f3"
      },
      "cell_type": "markdown",
      "source": "# Loading of training/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "24d7f3d982bfa582b222f012129acdda55282b6d"
      },
      "cell_type": "markdown",
      "source": "# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b18c1f50cefd7504eae7e7b9605be3814c7cad6d"
      },
      "cell_type": "code",
      "source": "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "A Jupyter Widget",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fbe71495322b437083bd3df08ab169de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "86620c6a070571895f4f36ec050a25803915ed74"
      },
      "cell_type": "code",
      "source": "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "A Jupyter Widget",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06a3d859e8ea436a9726c821ae06a8f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": "\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "1137f0a009f10b5f69e4dade5f689e744e9ce1d6"
      },
      "cell_type": "markdown",
      "source": "# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "18d2aa182a44c65a87c75f41047c653a79bc1c3f"
      },
      "cell_type": "code",
      "source": "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "2b13d1ecc7004832e8e042d034922796263054b7"
      },
      "cell_type": "code",
      "source": "def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5e66ff4809ea2f9a679b7ddbda5028dc324137a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dd39993eb2c7e77e5ce2d3388ea8ff1d581a670",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2391c568019151b098a002937516bb77a506f403"
      },
      "cell_type": "markdown",
      "source": "# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ae7b7011b7de3caed58f9ca3939df15ffa319ad",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "14835b3e0eafd3a1c0e3a1f18a2e7979e75d3fa3"
      },
      "cell_type": "markdown",
      "source": "# Show some example images"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "1a6bc85ee458f72c0917edf77895d5abc5eaf3ee",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "00655e32f93f96ebd90dbe94e35ee052f52217cd"
      },
      "cell_type": "markdown",
      "source": "# Create train/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d3c3157512d11e71ac74ce51a937b85bedfe1d1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2f1ab00f03e71e6d7f9b2214408b5a9779fc235",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "63ac58ab47921b4e4f54102e2c8b85fa318225f1"
      },
      "cell_type": "markdown",
      "source": "# Build model"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "9f9ce8801f56af6b418e9b47efd5099ef21acca2"
      },
      "cell_type": "code",
      "source": "base_model = keras.applications.vgg16.VGG16(\n    include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n\nx = base_model.output\nx = Conv2D(1024, (3, 3), activation=\"relu\", padding=\"same\")(x)\nx = keras.layers.Reshape((128, 128, 1))(x)\npredictions = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 128, 128, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 4, 4, 1024)        4719616   \n_________________________________________________________________\nreshape_3 (Reshape)          (None, 128, 128, 1)       0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 128, 128, 1)       2         \n=================================================================\nTotal params: 19,434,306\nTrainable params: 19,434,306\nNon-trainable params: 0\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3399029adb039b049e3d6ca01fef30ed8653482b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7ded4adc1757c88a1bea59ea36b1a9f7941bd28",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 128, 128, 3)       0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 4, 4, 1024)        4719616   \n_________________________________________________________________\nreshape_3 (Reshape)          (None, 128, 128, 1)       0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 128, 128, 1)       2         \n=================================================================\nTotal params: 19,434,306\nTrainable params: 4,719,618\nNon-trainable params: 14,714,688\n_________________________________________________________________\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c007157c2fd3d7dadcaeee2a6376351852d1e565"
      },
      "cell_type": "markdown",
      "source": "# Data augmentation"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "88b3f57eac3ec3719b401730dc6d8d2d89d09ccc"
      },
      "cell_type": "code",
      "source": "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "9198e4dd4291b11592981a6a7525c67b8bcd88ed"
      },
      "cell_type": "code",
      "source": "print(x_train.shape)\nprint(y_train.shape)",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(6400, 128, 128, 1)\n(6400, 128, 128, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "4faf8ef7f140069502ab76ec2dae93891ddb31f6"
      },
      "cell_type": "markdown",
      "source": "### images need 3 channels for VGG16"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "3fbf756c1b2e80d3e7e0b5a4b6e365dda122d01c"
      },
      "cell_type": "code",
      "source": "def expand_images_to_3_channels(images):\n    return np.stack((images.squeeze(),)*3, -1)",
      "execution_count": 52,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "353cdcd0e8a13d8c06c4f2998ca49e485c9aebb7"
      },
      "cell_type": "code",
      "source": "x_train = expand_images_to_3_channels(x_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "ffeeee203e4467ba8e135d0f5419b95a26052046"
      },
      "cell_type": "code",
      "source": "x_valid = expand_images_to_3_channels(x_valid)",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1ebb9b834aefdfb07a2ba8101657977c31642430"
      },
      "cell_type": "code",
      "source": "print(x_train.shape)\nprint(y_train.shape)\n\nprint(x_valid.shape)\nprint(y_valid.shape)",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(6400, 128, 128, 3)\n(6400, 128, 128, 1)\n(800, 128, 128, 3)\n(800, 128, 128, 1)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7040f72549212dd4f71c13dfbd8bf013481ea369",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f5a6b1abaa4681cba3b608bc5f33cf260370d82a"
      },
      "cell_type": "markdown",
      "source": "# Training"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1773642758da7b4480e0e48c045bd01ea3684ae",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\"./keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.00001, verbose=1)\n\nepochs = 200\nbatch_size = 32\n\nhistory = model.fit(x_train, y_train,\n                    validation_data=[x_valid, y_valid], \n                    epochs=epochs,\n                    batch_size=batch_size,\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr])",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 6400 samples, validate on 800 samples\nEpoch 1/20\n6400/6400 [==============================] - 31s 5ms/step - loss: 0.6708 - acc: 0.7467 - val_loss: 0.6504 - val_acc: 0.7471\n\nEpoch 00001: val_loss improved from inf to 0.65044, saving model to ./keras.model\nEpoch 2/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.6338 - acc: 0.7489 - val_loss: 0.6198 - val_acc: 0.7473\n\nEpoch 00002: val_loss improved from 0.65044 to 0.61978, saving model to ./keras.model\nEpoch 3/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.6074 - acc: 0.7490 - val_loss: 0.5979 - val_acc: 0.7473\n\nEpoch 00003: val_loss improved from 0.61978 to 0.59793, saving model to ./keras.model\nEpoch 4/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5892 - acc: 0.7490 - val_loss: 0.5833 - val_acc: 0.7473\n\nEpoch 00004: val_loss improved from 0.59793 to 0.58326, saving model to ./keras.model\nEpoch 5/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5771 - acc: 0.7490 - val_loss: 0.5738 - val_acc: 0.7473\n\nEpoch 00005: val_loss improved from 0.58326 to 0.57375, saving model to ./keras.model\nEpoch 6/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5693 - acc: 0.7490 - val_loss: 0.5679 - val_acc: 0.7473\n\nEpoch 00006: val_loss improved from 0.57375 to 0.56787, saving model to ./keras.model\nEpoch 7/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5645 - acc: 0.7490 - val_loss: 0.5643 - val_acc: 0.7473\n\nEpoch 00007: val_loss improved from 0.56787 to 0.56428, saving model to ./keras.model\nEpoch 8/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5615 - acc: 0.7491 - val_loss: 0.5621 - val_acc: 0.7473\n\nEpoch 00008: val_loss improved from 0.56428 to 0.56215, saving model to ./keras.model\nEpoch 9/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5599 - acc: 0.7491 - val_loss: 0.5610 - val_acc: 0.7473\n\nEpoch 00009: val_loss improved from 0.56215 to 0.56104, saving model to ./keras.model\nEpoch 10/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5590 - acc: 0.7491 - val_loss: 0.5605 - val_acc: 0.7473\n\nEpoch 00010: val_loss improved from 0.56104 to 0.56047, saving model to ./keras.model\nEpoch 11/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5585 - acc: 0.7491 - val_loss: 0.5602 - val_acc: 0.7473\n\nEpoch 00011: val_loss improved from 0.56047 to 0.56022, saving model to ./keras.model\nEpoch 12/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5583 - acc: 0.7491 - val_loss: 0.5601 - val_acc: 0.7473\n\nEpoch 00012: val_loss improved from 0.56022 to 0.56007, saving model to ./keras.model\nEpoch 13/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5582 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7474\n\nEpoch 00013: val_loss improved from 0.56007 to 0.56002, saving model to ./keras.model\nEpoch 14/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5581 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7474\n\nEpoch 00014: val_loss improved from 0.56002 to 0.56001, saving model to ./keras.model\nEpoch 15/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5581 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7474\n\nEpoch 00015: val_loss improved from 0.56001 to 0.56000, saving model to ./keras.model\nEpoch 16/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5581 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7474\n\nEpoch 00016: val_loss did not improve\nEpoch 17/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5581 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7473\n\nEpoch 00017: val_loss did not improve\n\nEpoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\nEpoch 18/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5581 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7474\n\nEpoch 00018: val_loss did not improve\nEpoch 19/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5581 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7474\n\nEpoch 00019: val_loss did not improve\nEpoch 20/20\n6400/6400 [==============================] - 28s 4ms/step - loss: 0.5581 - acc: 0.7491 - val_loss: 0.5600 - val_acc: 0.7474\n\nEpoch 00020: val_loss improved from 0.56000 to 0.55999, saving model to ./keras.model\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42e9ef3c4e0a2bb2539e5e51740ba6bfc092d37c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation accuracy\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c824f6bca47f051500966c433ce7fb5a9528f6d7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = load_model(\"./keras.model\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0f168318eadb324daa8c020f0e3e0a24d82a464f"
      },
      "cell_type": "markdown",
      "source": "# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16cbfe2fee11a8b13b96ce78161ce19b5e5a0c46",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3b1198b6fb7369c3cfb70e68cd1b78d36aa188bc",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "fd973023204ebf921fe1f23748856e6a6f692aa4"
      },
      "cell_type": "markdown",
      "source": "# Scoring\nScore the model and do a threshold optimization by the best IoU."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d261beec66b6867ac0d5c94684f12aa08b70d638"
      },
      "cell_type": "code",
      "source": "# src: https://www.kaggle.com/aglotero/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection / union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp / (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85f6d9567cec0ef8976730a6834b6569b6e108a0",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "183d37ad32bc2f1f0d17a9538702c45a826ccefc",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ced29761f2d1760245112a30a7abd4783b373dd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "423b3268c580dc1eae84f54deeeb0f691eff6028"
      },
      "cell_type": "markdown",
      "source": "# Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40c263765ac6d53a8c0c1361ff1e6f061eecf825",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "max_images = 60\ngrid_width = 15\ngrid_height = int(max_images / grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i / grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "332a614c0ae837c115ec6563f355753ffbb8cd83"
      },
      "cell_type": "markdown",
      "source": "# Submission\nLoad, predict and submit the test image predictions."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72128add82c6853441671fde67e7e66601a01787",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Source https://www.kaggle.com/bguberfain/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "3ecb152b492c7126d12c5ef2c701eec8ea3d86f1",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "x_test = np.array([upsample(np.array(load_img(\"../input/test/images/{}.png\".format(idx), grayscale=True))) / 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f278d0b87320c117b4ed7c116a991782b82ba5a7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "preds_test = model.predict(x_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "113f816f9db8b87ca7f6845fe6e61328ab606f41",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "4243166f91c4bcb4da00208f4f53dd912dbb429f"
      },
      "cell_type": "code",
      "source": "sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "bd6ce9b4d5fc80a2502a43e80299d628fb5ffc42"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}